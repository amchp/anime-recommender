{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4B3-4j3AgiG"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCZXRiuqwdvB"
   },
   "source": [
    "## a. Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpNxAfA_7G9T"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import expr, rank, col, collect_list\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cgE-PwC9-6O"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"sample-user-filtered-2023.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIOS7iT3vaJY"
   },
   "outputs": [],
   "source": [
    "df = df.orderBy(\"user_id\", \"anime_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ul-CzqPP-I5D",
    "outputId": "efbd00f3-d367-4a16-f7d3-9f967569fce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|anime_id|rating|\n",
      "+-------+--------+------+\n",
      "|      1|      19|     9|\n",
      "|      1|      20|    10|\n",
      "|      1|      21|     9|\n",
      "|      1|     154|     8|\n",
      "|      1|     199|    10|\n",
      "|      1|     269|     7|\n",
      "|      1|     442|     7|\n",
      "|      1|     481|     8|\n",
      "|      1|    1535|     9|\n",
      "|      1|    1575|    10|\n",
      "|      1|    1735|    10|\n",
      "|      1|    2472|     7|\n",
      "|      1|    4224|     9|\n",
      "|      1|    5081|     8|\n",
      "|      1|    5114|    10|\n",
      "|      1|    6547|     6|\n",
      "|      1|    7674|     7|\n",
      "|      1|    9253|    10|\n",
      "|      1|    9919|     8|\n",
      "|      1|   10087|     7|\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pACeoXnYB4aX",
    "outputId": "d7b0a8b8-3ac6-4246-eb31-53dcc025dca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- anime_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iolUxDWV6Osl"
   },
   "outputs": [],
   "source": [
    "user_window = Window.partitionBy(\"user_id\").orderBy(col(\"anime_id\").desc())\n",
    "df = df.withColumn(\"num_animes\", expr(\"count(*) over (partition by user_id)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1xZzpHE8PW4"
   },
   "outputs": [],
   "source": [
    "percent_animes_to_mask = 0.3\n",
    "\n",
    "df_final = df.withColumn(\"num_animes_to_mask\", (col(\"num_animes\") * percent_animes_to_mask).cast(\"int\"))\n",
    "df_final = df_final.withColumn(\"anime_rank\", rank().over(user_window))\n",
    "\n",
    "indexer_user = StringIndexer(inputCol='user_id', outputCol='user_index').setHandleInvalid(\"keep\")\n",
    "indexer_anime = StringIndexer(inputCol='anime_id', outputCol='anime_index').setHandleInvalid(\"keep\")\n",
    "\n",
    "df_final = indexer_user.fit(df_final).transform(df_final)\n",
    "df_final = indexer_anime.fit(df_final).transform(df_final)\n",
    "\n",
    "df_final = df_final.withColumn('user_index', df_final['user_index'].cast('integer'))\n",
    "df_final = df_final.withColumn('anime_index', df_final['anime_index'].cast('integer'))\n",
    "\n",
    "train_data = df_final.filter(col(\"anime_rank\") > col(\"num_animes_to_mask\"))\n",
    "test_data = df_final.filter(col(\"anime_rank\") <= col(\"num_animes_to_mask\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQBxaRckwUTs",
    "outputId": "5adbc0bb-8a07-4adc-9cec-50324d2aa716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank:  10\n",
      "MaxIter:  15\n",
      "RegParam:  0.15\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "\n",
    "als = ALS(userCol='user_index', itemCol='anime_index', ratingCol='rating',\n",
    "          coldStartStrategy='drop', nonnegative=True)\n",
    "\n",
    "\n",
    "param_grid = ParamGridBuilder().addGrid(\n",
    "    als.rank, [1, 10, 20]\n",
    ").addGrid(\n",
    "    als.maxIter, [15]\n",
    ").addGrid(\n",
    "    als.regParam, [.05, .15]\n",
    ").build()\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "cv = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3)\n",
    "\n",
    "model = cv.fit(train_data)\n",
    "\n",
    "best_model = model.bestModel\n",
    "print('rank: ', best_model.rank)\n",
    "print('MaxIter: ', best_model._java_obj.parent().getMaxIter())\n",
    "print('RegParam: ', best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3BRAsoJQ9KzK"
   },
   "outputs": [],
   "source": [
    "final_als = ALS(\n",
    "    userCol='user_index',\n",
    "    itemCol='anime_index',\n",
    "    ratingCol='rating',\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=True,\n",
    "    rank=10,\n",
    "    maxIter=20,\n",
    "    regParam=0.15\n",
    ")\n",
    "\n",
    "best_model = als.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlbidXnF7SGi",
    "outputId": "78503654-76d9-4a3b-9115-aacb77af1b4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 1.3548028972766433\n",
      "Mean Absolute Error (MAE): 1.0377965388827288\n",
      "Mean Squared Error (MSE): 1.8354908904691867\n",
      "R² (coefficient of determination): 0.2703084700615269\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.transform(test_data)\n",
    "predictions = predictions.withColumn(\"prediction\", expr(\"CASE WHEN prediction < 1 THEN 1 WHEN prediction > 10 THEN 10 ELSE prediction END\"))\n",
    "evaluator = RegressionEvaluator(labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "metric_names = ['rmse', 'mae', 'mse', 'r2']\n",
    "\n",
    "for metric in metric_names:\n",
    "    evaluator.setMetricName(metric)\n",
    "    metrics[metric] = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {metrics['rmse']}\")\n",
    "print(f\"Mean Absolute Error (MAE): {metrics['mae']}\")\n",
    "print(f\"Mean Squared Error (MSE): {metrics['mse']}\")\n",
    "print(f\"R² (coefficient of determination): {metrics['r2']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vf58Uk6-D4ih",
    "outputId": "9bb603bd-0a0e-4357-caa0-11ae46fba128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.0208\n",
      "NDCG at K: 0.1272\n",
      "Precision at K: 0.1207\n",
      "Recall at K: 0.0420\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def evaluate_als_recommendations(model, test_data, k=10):\n",
    "    user_subset = test_data.select('user_index').distinct()\n",
    "    recommendations = model.recommendForUserSubset(user_subset, k)\n",
    "\n",
    "    predicted_rankings = recommendations.rdd.map(\n",
    "        lambda row: (row.user_index, [int(x.anime_index) for x in row.recommendations])\n",
    "    )\n",
    "\n",
    "    actual_rankings = test_data.groupBy('user_index').agg(\n",
    "        F.collect_list('anime_index').alias('actual_items')\n",
    "    ).rdd.map(\n",
    "        lambda row: (row.user_index, [int(x) for x in row.actual_items])\n",
    "    )\n",
    "\n",
    "    prediction_truth = predicted_rankings.join(actual_rankings)\n",
    "\n",
    "    metrics = RankingMetrics(prediction_truth.map(lambda x: (x[1][0], x[1][1])))\n",
    "\n",
    "    # Calculate various metrics\n",
    "    results = {\n",
    "        'MAP': metrics.meanAveragePrecision,\n",
    "        'NDCG at K': metrics.ndcgAt(k),\n",
    "        'Precision at K': metrics.precisionAt(k),\n",
    "        'Recall at K': metrics.recallAt(k)\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Use the evaluation function\n",
    "metrics_results = evaluate_als_recommendations(best_model, test_data, k=10)\n",
    "\n",
    "# Print results\n",
    "for metric_name, value in metrics_results.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "w5gZgvvLPDHy"
   },
   "outputs": [],
   "source": [
    "userRecs = best_model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqRxZWM0LmeZ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def format_recommendations_for_export(userRecs):\n",
    "    flattened = userRecs.select(\n",
    "        'user_index',\n",
    "        F.posexplode('recommendations').alias('position', 'rec')\n",
    "    ).select(\n",
    "        'user_index',\n",
    "        'position',\n",
    "        'rec.anime_index',\n",
    "        'rec.rating'\n",
    "    )\n",
    "\n",
    "    pivoted = flattened.groupBy('user_index').pivot('position').agg(\n",
    "        F.first('anime_index').alias('anime_index'),\n",
    "        F.first('rating').alias('predicted_rating')\n",
    "    )\n",
    "\n",
    "    for i in range(10):\n",
    "        pivoted = (pivoted\n",
    "            .withColumnRenamed(f'{i}_anime_index', f'recommendation_{i+1}_anime_id')\n",
    "            .withColumnRenamed(f'{i}_predicted_rating', f'recommendation_{i+1}_score'))\n",
    "\n",
    "    final_df = pivoted.orderBy('user_index')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "formatted_recs = format_recommendations_for_export(userRecs)\n",
    "\n",
    "formatted_recs.toPandas().to_csv('anime_recommendations.csv', index=False)\n",
    "\n",
    "formatted_recs.coalesce(1).write.mode('overwrite').option('header', 'true').csv('anime_recommendations_spark')\n",
    "\n",
    "formatted_recs.show(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
