{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python3\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Content_Based_Recommendation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, split, when, mean, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"fsspec\")\n",
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pandas = pd.read_csv(\"s3://proyectommds/anime-dataset-2023.csv\")\n",
    "df_anime = spark.createDataFrame(df_pandas)\n",
    "df_anime.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el esquema del DataFrame\n",
    "df_anime.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de filas y columnas\n",
    "num_rows = df_anime.count()\n",
    "num_cols = len(df_anime.columns)\n",
    "print(f\"Número de filas: {num_rows}, Número de columnas: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas y sus tipos\n",
    "for col, dtype in df_anime.dtypes:\n",
    "    print(f\"Columna: {col}, Tipo: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar animes por tipo\n",
    "df_type=df_anime.groupBy(\"Type\").count().orderBy(F.desc(\"count\"))\n",
    "df_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, split, explode\n",
    "# Reemplazar \"UNKNOWN\" por NULL (vacío) en la columna \"Genres\"\n",
    "df_anime = df_anime.withColumn(\"Genres\", when(col(\"Genres\") == \"UNKNOWN\", None).otherwise(col(\"Genres\")))\n",
    "# Dividir la columna \"Genres\" en una lista de géneros\n",
    "df_genres = df_anime.withColumn(\"GenreArray\", split(col(\"Genres\"), \", \"))\n",
    "# Explode: Expandir los géneros en filas individuales\n",
    "df_exploded = df_genres.withColumn(\"Genre\", explode(col(\"GenreArray\")))\n",
    "# Contar las ocurrencias de cada género\n",
    "genre_counts = df_exploded.groupBy(\"Genre\").count().orderBy(col(\"count\").desc())\n",
    "# Mostrar el resultado\n",
    "genre_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar filas con valor \"UNKNOWN\"\n",
    "unknown_score = df_anime.filter(col(\"Score\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Score 'UNKNOWN': {unknown_score}\")\n",
    "unknown_episodes = df_anime.filter(col(\"Episodes\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Episodes 'UNKNOWN': {unknown_episodes}\")\n",
    "unknown_type = df_anime.filter(col(\"Type\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Type 'UNKNOWN': {unknown_type}\")\n",
    "unknown_aired = df_anime.filter(col(\"Aired\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Aired 'UNKNOWN': {unknown_aired}\")\n",
    "unknown_status = df_anime.filter(col(\"Status\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Status 'UNKNOWN': {unknown_status}\")\n",
    "unknown_studios = df_anime.filter(col(\"Studios\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Studios 'UNKNOWN': {unknown_studios}\")\n",
    "unknown_source = df_anime.filter(col(\"Source\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Source 'UNKNOWN': {unknown_source}\")\n",
    "unknown_duration = df_anime.filter(col(\"Duration\") == \"Unknown\").count()\n",
    "print(f\"Número de animes con Duration 'UNKNOWN': {unknown_duration}\")\n",
    "unknown_rating = df_anime.filter(col(\"Rating\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Rating 'UNKNOWN': {unknown_rating}\")\n",
    "unknown_pop = df_anime.filter(col(\"Popularity\") == \"Unknown\").count()\n",
    "print(f\"Número de animes con Popularity 'UNKNOWN': {unknown_pop}\")\n",
    "unknown_mem = df_anime.filter(col(\"Members\") == \"UNKNOWN\").count()\n",
    "print(f\"Número de animes con Members 'UNKNOWN': {unknown_mem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodes a tipo float\n",
    "df_anime = df_anime.withColumn(\"Episodes\", col(\"Episodes\").cast(\"float\"))\n",
    "# Filtrar las filas que tienen valores numéricos\n",
    "df_numeric_episodes = df_anime.filter(col(\"Episodes\").isNotNull())\n",
    "# Calcular la media de los episodios\n",
    "mean_episodes = df_numeric_episodes.select(mean(col(\"Episodes\"))).collect()[0][0]\n",
    "# Reemplazar los valores \"UNKNOWN\" por la media calculada\n",
    "df_anime = df_anime.withColumn(\"Episodes\", when(col(\"Episodes\").isNull(), mean_episodes).otherwise(col(\"Episodes\")))\n",
    "# Ordenar por Score en orden descendente, seleccionar columnas y limitar a 15 resultados\n",
    "top_15_animes = (df_anime.orderBy(col(\"Episodes\").desc()).select(\"Name\", \"Episodes\").limit(15))\n",
    "# Mostrar el resultado\n",
    "top_15_animes.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar \"UNKNOWN\" por NULL\n",
    "df_anime = df_anime.withColumn(\"Type\", when(col(\"Type\") == \"UNKNOWN\", None).otherwise(col(\"Type\")))\n",
    "df_anime = df_anime.withColumn(\"Duration\", when(col(\"Duration\") == \"Unknown\", None).otherwise(col(\"Duration\")))\n",
    "df_anime = df_anime.withColumn(\"Rating\", when(col(\"Rating\") == \"UNKNOWN\", None).otherwise(col(\"Rating\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame con las columnas definitivas\n",
    "data = df_anime.select(\"anime_id\", \"Name\", \"Genres\", \"Synopsis\", \"Type\", \"Episodes\", \"Aired\", \n",
    "                       \"Status\", \"Source\", \"Duration\", \"Rating\", \"Popularity\", \"Members\")\n",
    "\n",
    "for col, dtype in data.dtypes:\n",
    "    print(f\"Columna: {col}, Tipo: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Variables tipo numéricas\n",
    "columns_to_normalize = [\"Episodes\", \"Popularity\", \"Members\"]\n",
    "for column in columns_to_normalize:\n",
    "    data = data.withColumn(column, col(column).cast(\"float\"))\n",
    "# Vectorizar columnas numéricas\n",
    "assembler = VectorAssembler(inputCols=columns_to_normalize, outputCol=\"features_vector\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Función UDF para obtener la longitud de un vector\n",
    "def get_vector_size(v):\n",
    "    return len(v)\n",
    "# Registrar la UDF\n",
    "get_vector_size_udf = udf(get_vector_size, IntegerType())\n",
    "# Aplicar la UDF para obtener el tamaño del vector\n",
    "data = data.withColumn(\"vector_size\", get_vector_size_udf(\"features_vector\"))\n",
    "\n",
    "# Verificar los resultados\n",
    "data.select(\"features_vector\", \"vector_size\").show(truncate=False)\n",
    "\n",
    "# Eliminar la columna de verificación\n",
    "data = data.drop(\"vector_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features_vector\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(data)\n",
    "data = scaler_model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Concatenar todas las columnas tipo string\n",
    "data = data.withColumn(\"combined_text\", concat_ws(\" \", \"Name\", \"Genres\", \"Synopsis\", \"Type\", \n",
    "                                                  \"Aired\", \"Status\", \"Source\", \"Duration\", \"Rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, split\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Texto en minusculas y eliminar caracteres especiales\n",
    "data = data.withColumn(\"processed_text\", lower(col(\"combined_text\")))\n",
    "data = data.withColumn(\"processed_text\", regexp_replace(col(\"processed_text\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "\n",
    "# Convertir la columna 'processed_text' en un array de palabras\n",
    "data = data.withColumn(\"words\", split(col(\"processed_text\"), \" \"))\n",
    "\n",
    "# Eliminar stopwords \n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_text\")\n",
    "data = remover.transform(data)\n",
    "\n",
    "data.select(\"processed_text\", \"words\", \"filtered_text\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Crear un HashingTF para vectorizar el texto procesado\n",
    "hashingTF = HashingTF(inputCol=\"filtered_text\", outputCol=\"raw_features\", numFeatures=2000)\n",
    "\n",
    "# Calcular TF-IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Pipeline para las transformaciones\n",
    "pipeline = Pipeline(stages=[hashingTF, idf])\n",
    "\n",
    "model = pipeline.fit(data)\n",
    "data = model.transform(data)\n",
    "\n",
    "data.select(\"anime_id\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "\n",
    "# Función UDF para calcular la similitud de coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Convertir vectores de SparseVector a arrays\n",
    "    vec1_array = np.array(vec1.toArray())\n",
    "    vec2_array = np.array(vec2.toArray())\n",
    "    \n",
    "    # Calcular el producto punto y las longitudes de los vectores\n",
    "    dot_product = np.dot(vec1_array, vec2_array)\n",
    "    norm_a = np.linalg.norm(vec1_array)\n",
    "    norm_b = np.linalg.norm(vec2_array)\n",
    "    \n",
    "    # Calcular la similitud de coseno\n",
    "    return float(dot_product / (norm_a * norm_b)) if norm_a and norm_b else 0.0\n",
    "\n",
    "# Registrar la UDF de similitud de coseno\n",
    "cosine_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# matriz de similitudes\n",
    "cosine_sim_matrix = data.alias(\"df1\").join(data.alias(\"df2\"), col(\"df1.anime_id\") != col(\"df2.anime_id\")) \\\n",
    "    .withColumn(\"cosine_sim\", cosine_udf(col(\"df1.features\"), col(\"df2.features\")))\n",
    "\n",
    "cosine_sim_matrix.select(\"df1.anime_id\", \"df2.anime_id\", \"cosine_sim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Función para obtener recomendaciones basadas en la similitud de coseno\n",
    "def get_recommendations(anime_name, cosine_sim_matrix, df, top_n=10):\n",
    "    # Buscar el anime_id basado en el nombre\n",
    "    anime_id_row = df.filter(col(\"Name\") == anime_name).select(\"anime_id\").first()\n",
    "    \n",
    "    if anime_id_row is None:\n",
    "        return \"Anime not found\"\n",
    "    \n",
    "    anime_id = anime_id_row[\"anime_id\"]\n",
    "    \n",
    "    # Filtrar las similitudes para el anime_id dado\n",
    "    recommendations = cosine_sim_matrix.filter(col(\"df1.anime_id\") == anime_id) \\\n",
    "        .orderBy(desc(\"cosine_sim\")) \\\n",
    "        .select(\"df2.anime_id\", \"df2.Name\", \"cosine_sim\")\n",
    "    \n",
    "    # Mostrar las top_n recomendaciones\n",
    "    recommendations = recommendations.limit(top_n)\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo:\n",
    "recommended_animes = get_recommendations(anime_name=\"Tensei shitara Slime Datta Ken\", cosine_sim_matrix=cosine_sim_matrix, df=data, top_n=20)\n",
    "recommended_animes.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
